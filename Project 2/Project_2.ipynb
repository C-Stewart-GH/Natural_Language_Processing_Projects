{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "907dc0ee",
   "metadata": {},
   "source": [
    "### Cameron Stewart\n",
    "# HW2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f01cc48f",
   "metadata": {},
   "source": [
    "### 1.\tIn Python, create a method for scoring the vocabulary size of a text, and normalize the score from 0 to 1. It does not matter what method you use for normalization as long as you explain it in a short paragraph. \n",
    "Some relevant resources that you can leverage:                   \n",
    "https://docs.tibco.com/pub/spotfire/6.5.0/doc/html/norm/norm_scale_between_0_and_1.htm\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MinMaxScaler.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ba78b871",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vocabulary_normalizer(vocab_size):\n",
    "    min_val=2500\n",
    "    max_val=30030\n",
    "    clean_vocab_size=max(min(vocab_size,max_val),min_val)\n",
    "    return (clean_vocab_size - min_val) / (max_val - min_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0015ccaf",
   "metadata": {},
   "source": [
    "To normalize vocabulary size between 0 and 1, we would ideally have a large sample of book vocabulary sizes to reference. Since we do not, I searched for examples on the internet to find the estimated min and max. Based on findings from this site (https://www.grammarly.com/blog/7-novels-to-read-for-a-better-vocabulary/#:~:text=Ulysses,English%20novels%20of%20all%20time.), Ulysses is one of the most difficult novels to read and represents near the 'max' for unique words in an English novel at 30,030 unique words. We will use this as our max to normalize our vocabulary size scale. I did not find a good source for the minimum, but did find the a minimum word count for a novel to be ~40,000. Based on this, I will estimate my lower bound of vocabulary size to be 2,500. We will normalize the studied book's vocabulary size between the defined min and max values. There is also an boundary checker to ensure that rare books outside this range can't fall outside of 0 and 1."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c5fc5f72",
   "metadata": {},
   "source": [
    "### 2.\tAfter consulting section 3.2 in chapter 1 of Bird-Klein, create a method for scoring the long-word vocabulary size of a text, and likewise normalize (and explain) the scoring as in step 1 above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bf764440",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to count unique long words over 7 letters that appear over 7 times\n",
    "def unique_long_word_counter(tokenized_text):\n",
    "    fdist = FreqDist(tokenized_text)\n",
    "    return len([x for x in set(tokenized_text) if len(x) > 7 and fdist[x] > 7])\n",
    "\n",
    "#Function to normalize unique long word count\n",
    "def long_word_vocabulary_normalizer(lw_vocab_size):\n",
    "    min_val=40\n",
    "    max_val=800\n",
    "    clean_lw_vocab_size=max(min(lw_vocab_size,max_val),min_val)  \n",
    "    return (clean_lw_vocab_size - min_val) / (max_val - min_val)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ca4aa4be",
   "metadata": {},
   "source": [
    "Using Bird-Klein's example, we defined a 'long-word' as anything over 7 characters that appeared more than 7 times. The first function counts the unique words that meet this long word criteria. Once again for the normalization, we would prefer to have a large sample to help us define the min and max values of long word vocabulary size. Without this and no clear reference list online, I made a subjective decision to set the min to 40 and the max to 800. This should be updated when more data is collected. Also, a potential concern that should be monitored is that hyphenated words are currently included."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "857bf2ae",
   "metadata": {},
   "source": [
    "### 3.\tNow create a “text difficulty score” by combining the lexical diversity score from homework 1, and your normalized score of vocabulary size and long-word vocabulary size, in equal weighting. Explain what you see when this score is applied to same graded texts you used in homework 1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd1e8e04",
   "metadata": {},
   "source": [
    "Load in libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d6df6889",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load Required Functions\n",
    "from urllib import request\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "from nltk.probability import FreqDist"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d309c882",
   "metadata": {},
   "source": [
    "Load in HW1 lexical diversity function to get vocab size/token count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "77a1245f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defined function to calculate lexical diversity from last HW\n",
    "def lexical_diversity(text):\n",
    "    return len(set(text)) / len(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "254d0dd3",
   "metadata": {},
   "source": [
    "Create function to output prints of key statistics from HW1 and normalized requested values for HW2. This function cleans the text to only include the contents of the story instead of extraneous information. This function incorporates the lexical diversity, unique_long_word_counter, long_word_vocabulary_normalizer, and vocabulary_normalizer functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8f739d63",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defined function to print out all statistics from HW1 and requested normalized values for HW2\n",
    "def print_book_statistics(url,reading_level,start_text,end_text):\n",
    "    #Print Reading Level\n",
    "    print('Reading Level:',reading_level)\n",
    "\n",
    "    #Fetch text through the URL and tokenize the text\n",
    "    response = request.urlopen(url)\n",
    "    raw = response.read().decode('utf8')\n",
    "    tokens = word_tokenize(raw)\n",
    "    print('Original Token Count:',len(tokens))\n",
    "\n",
    "    #Manually identify start and end phrases of book and use their index to clean the raw file of unnecessary text\n",
    "    start=raw.find(start_text)\n",
    "    end=raw.rfind(end_text)\n",
    "    clean_raw=raw[start:end]\n",
    "    clean_tokens = word_tokenize(clean_raw)\n",
    "    print('Cleaned Token Count:',len(clean_tokens),'\\n')\n",
    "\n",
    "    #Create NLTK Text so we can use NLTK methods on the text (verified token count remained the same)\n",
    "    text = nltk.Text(clean_tokens)\n",
    "\n",
    "    #Calculate Vocabulary Size\n",
    "    vocab_size=len(set(text))\n",
    "    print('Vocabulary Size:',vocab_size)\n",
    "    \n",
    "    #Long Word Vocabulary Size\n",
    "    unique_lw_count=unique_long_word_counter(clean_tokens)\n",
    "    print('Long Word Vocabulary Size',unique_lw_count,'\\n')\n",
    "    \n",
    "    #Normalized Values\n",
    "    lex_div=lexical_diversity(text)\n",
    "    norm_vocab_size=vocabulary_normalizer(vocab_size)\n",
    "    norm_lw_vocab_size=long_word_vocabulary_normalizer(unique_lw_count)\n",
    "    print('Lexical Diversity:',round(lex_div,2))\n",
    "    print('Normalized Vocabulary Size:', round(norm_vocab_size,2))\n",
    "    print('Normalized Long Word Vocabulary Size:',round(norm_lw_vocab_size,2))\n",
    "    print('Text Difficulty Score:', round((lex_div+norm_vocab_size+norm_lw_vocab_size)/3,2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a93e0bcd",
   "metadata": {},
   "source": [
    "### Below are the 3 books analyzed from HW1 with updated analysis for HW2:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bab3489",
   "metadata": {},
   "source": [
    "#### The Phoenix and the Carpet by E. Nesbit (Reading Level 5-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "90d73299",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading Level: 5-6\n",
      "Original Token Count: 82874\n",
      "Cleaned Token Count: 79211 \n",
      "\n",
      "Vocabulary Size: 6665\n",
      "Long Word Vocabulary Size 77 \n",
      "\n",
      "Lexical Diversity: 0.08\n",
      "Normalized Vocabulary Size: 0.15\n",
      "Normalized Long Word Vocabulary Size: 0.05\n",
      "Text Difficulty Score: 0.09\n"
     ]
    }
   ],
   "source": [
    "url = \"https://www.gutenberg.org/files/836/836-0.txt\"\n",
    "reading_level='5-6'\n",
    "start_text=\"CHAPTER 1. THE EGG\"\n",
    "end_text=\"End of the Project Gutenberg EBook\"\n",
    "print_book_statistics(url,reading_level,start_text,end_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fac109a5",
   "metadata": {},
   "source": [
    "#### Kidnapped by Robert Louis Stevenson (Reading Level 7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aebcbe53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading Level: 7\n",
      "Original Token Count: 103147\n",
      "Cleaned Token Count: 97506 \n",
      "\n",
      "Vocabulary Size: 7314\n",
      "Long Word Vocabulary Size 94 \n",
      "\n",
      "Lexical Diversity: 0.08\n",
      "Normalized Vocabulary Size: 0.17\n",
      "Normalized Long Word Vocabulary Size: 0.07\n",
      "Text Difficulty Score: 0.11\n"
     ]
    }
   ],
   "source": [
    "url = \"https://www.gutenberg.org/files/421/421-0.txt\"\n",
    "reading_level='7'\n",
    "start_text=\"CHAPTER I\"\n",
    "end_text=\"End of the Project Gutenberg EBook\"\n",
    "print_book_statistics(url,reading_level,start_text,end_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0978975",
   "metadata": {},
   "source": [
    "#### Pamela, or Virtue Rewarded by Samuel Richardson (Reading Level 10-12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fcd4b49c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading Level: 10-12\n",
      "Original Token Count: 272922\n",
      "Cleaned Token Count: 269161 \n",
      "\n",
      "Vocabulary Size: 9086\n",
      "Long Word Vocabulary Size 429 \n",
      "\n",
      "Lexical Diversity: 0.03\n",
      "Normalized Vocabulary Size: 0.24\n",
      "Normalized Long Word Vocabulary Size: 0.51\n",
      "Text Difficulty Score: 0.26\n"
     ]
    }
   ],
   "source": [
    "url = \"https://www.gutenberg.org/files/6124/6124-0.txt\"\n",
    "reading_level= '10-12'\n",
    "start_text=\"LETTER I\"\n",
    "end_text=\"End of Project Gutenberg's Pamela, or Virtue Rewarded\"\n",
    "print_book_statistics(url,reading_level,start_text,end_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e02a1d65",
   "metadata": {},
   "source": [
    "The text difficulty score follows a trend with the reading scores in my examples unlike the lexical diversity alone in the last HW. This is only a single sample size at each reading level, so we should proceed with caution assuming this methodology is effective. With a larger sample size for each reading level, we could refine the min and max normalization values for the normalized vocabulary size and normalized long word vocabulary size functions. We could also refine the weights for the inputs of the lexical diversity, normalized vocabulary size, and normalized long word vocabulary size functions. Overall, the findings show this method to be a more balanced approach to evaluating text difficulty than lexical diversity alone by balancing text length, vocabulary, and frequent use of more complex vocabulary."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ds_p37_r35]",
   "language": "python",
   "name": "conda-env-ds_p37_r35-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
