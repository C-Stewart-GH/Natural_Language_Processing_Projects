{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "907dc0ee",
   "metadata": {},
   "source": [
    "### Cameron Stewart\n",
    "# HW1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c990bed",
   "metadata": {},
   "source": [
    "### 1.\tInstall Python (if you don’t have it already) and install NLTK."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d596005a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load NLTK\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75a58277",
   "metadata": {},
   "source": [
    "Completed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc1b501e",
   "metadata": {},
   "source": [
    "### 2.\tFollow the instructions in chapter 1 of Bird-Klein for implementing a “lexical diversity” scoring routine."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f808772",
   "metadata": {},
   "source": [
    "Below, we use a pre-defined text in the nltk library (text3) to show how lexical diversity is calculated. We also define a function to be used in questions 3-4 to calculate lexical diversity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9fab7e03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Introductory Examples for the NLTK Book ***\n",
      "Loading text1, ..., text9 and sent1, ..., sent9\n",
      "Type the name of the text or sentence to view it.\n",
      "Type: 'texts()' or 'sents()' to list the materials.\n",
      "text1: Moby Dick by Herman Melville 1851\n",
      "text2: Sense and Sensibility by Jane Austen 1811\n",
      "text3: The Book of Genesis\n",
      "text4: Inaugural Address Corpus\n",
      "text5: Chat Corpus\n",
      "text6: Monty Python and the Holy Grail\n",
      "text7: Wall Street Journal\n",
      "text8: Personals Corpus\n",
      "text9: The Man Who Was Thursday by G . K . Chesterton 1908\n"
     ]
    }
   ],
   "source": [
    "#Load Book Examples\n",
    "from nltk.book import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a796ad9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lexical Diversity of text3: 0.06230453042623537\n",
      "Lexical Diversity of text3 with defined function: 0.06230453042623537\n"
     ]
    }
   ],
   "source": [
    "#Example of manual calculation of lexical diversity of pre-stored text3\n",
    "print('Lexical Diversity of text3:',len(set(text3)) / len(text3))\n",
    "\n",
    "#Defined function to calculate lexical diversity\n",
    "def lexical_diversity(text):\n",
    "    return len(set(text)) / len(text)\n",
    "\n",
    "#Proof function returns the same value as example\n",
    "print('Lexical Diversity of text3 with defined function:',lexical_diversity(text3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8df25a9f",
   "metadata": {},
   "source": [
    "### 3.\tGo to http://www.gutenberg.org/ebooks/bookshelf/215  and select texts of different grade levels (e.g., fourth reader, fifth reader et al) Report the lexical diversity score of each. Explain whether the result was surprising."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d60c122c",
   "metadata": {},
   "source": [
    "Below, we will analyze the lexical diversity of three books from the Gutenberg Project. Reading level was determined using Amazon Product Information for each book."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6dc0a992",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load Required Functions\n",
    "from urllib import request\n",
    "from nltk import word_tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8be39be7",
   "metadata": {},
   "source": [
    "#### The Phoenix and the Carpet by E. Nesbit (Reading Level 5-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0ea52c07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading Level: 5-6\n",
      "Original Token Count: 82874\n",
      "Cleaned Token Count: 79211\n",
      "Lexical Diversity: 0.08414235396598957\n",
      "Vocabulary Size: 6665\n"
     ]
    }
   ],
   "source": [
    "#Print Reading Level\n",
    "print('Reading Level: 5-6')\n",
    "\n",
    "#Fetch text through the URL and tokenize the text\n",
    "url = \"https://www.gutenberg.org/files/836/836-0.txt\"\n",
    "response = request.urlopen(url)\n",
    "raw = response.read().decode('utf8')\n",
    "tokens = word_tokenize(raw)\n",
    "print('Original Token Count:',len(tokens))\n",
    "\n",
    "#Manually identify start and end phrases of book and use their index to clean the raw file of unnecessary text\n",
    "start=raw.find(\"CHAPTER 1. THE EGG\")\n",
    "end=raw.rfind(\"End of the Project Gutenberg EBook\")\n",
    "clean_raw=raw[start:end]\n",
    "clean_tokens = word_tokenize(clean_raw)\n",
    "print('Cleaned Token Count:',len(clean_tokens))\n",
    "\n",
    "#Create NLTK Text so we can use NLTK methods on the text (verified token count remained the same)\n",
    "text = nltk.Text(clean_tokens)\n",
    "\n",
    "#Calculate Lexical Diversity\n",
    "print('Lexical Diversity:',lexical_diversity(text))\n",
    "\n",
    "#Calculate Vocabulary Size\n",
    "print('Vocabulary Size:',len(set(text)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0c65b93",
   "metadata": {},
   "source": [
    "#### Kidnapped by Robert Louis Stevenson (Reading Level 7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4d87ef30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading Level: 7\n",
      "Original Token Count: 103147\n",
      "Cleaned Token Count: 97506\n",
      "Lexical Diversity: 0.07501076856808812\n",
      "Vocabulary Size: 7314\n"
     ]
    }
   ],
   "source": [
    "#Print Reading Level\n",
    "print('Reading Level: 7')\n",
    "\n",
    "#Fetch text through the URL and tokenize the text\n",
    "url = \"https://www.gutenberg.org/files/421/421-0.txt\"\n",
    "response = request.urlopen(url)\n",
    "raw = response.read().decode('utf8')\n",
    "tokens = word_tokenize(raw)\n",
    "print('Original Token Count:',len(tokens))\n",
    "\n",
    "#Manually identify start and end phrases of book and use their index to clean the raw file of unnecessary text\n",
    "start=raw.find(\"CHAPTER I\")\n",
    "end=raw.rfind(\"End of the Project Gutenberg EBook\")\n",
    "clean_raw=raw[start:end]\n",
    "clean_tokens = word_tokenize(clean_raw)\n",
    "print('Cleaned Token Count:',len(clean_tokens))\n",
    "\n",
    "#Create NLTK Text so we can use NLTK methods on the text (verified token count remained the same)\n",
    "text = nltk.Text(clean_tokens)\n",
    "\n",
    "#Calculate Lexical Diversity\n",
    "print('Lexical Diversity:',lexical_diversity(text))\n",
    "\n",
    "#Calculate Vocabulary Size\n",
    "print('Vocabulary Size:',len(set(text)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0148f6ed",
   "metadata": {},
   "source": [
    "#### Pamela, or Virtue Rewarded by Samuel Richardson (Reading Level 10-12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "497da29a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading Level: 10-12\n",
      "Original Token Count: 272922\n",
      "Cleaned Token Count: 269161\n",
      "Lexical Diversity: 0.03375674782007795\n",
      "Vocabulary Size: 9086\n"
     ]
    }
   ],
   "source": [
    "#Print Reading Level\n",
    "print('Reading Level: 10-12')\n",
    "\n",
    "#Fetch text through the URL and tokenize the text\n",
    "url = \"https://www.gutenberg.org/files/6124/6124-0.txt\"\n",
    "response = request.urlopen(url)\n",
    "raw = response.read().decode('utf8')\n",
    "tokens = word_tokenize(raw)\n",
    "print('Original Token Count:',len(tokens))\n",
    "\n",
    "#Manually identify start and end phrases of book and use their index to clean the raw file of unnecessary text\n",
    "start=raw.find(\"LETTER I\")\n",
    "end=raw.rfind(\"End of Project Gutenberg's Pamela, or Virtue Rewarded\")\n",
    "clean_raw=raw[start:end]\n",
    "clean_tokens = word_tokenize(clean_raw)\n",
    "print('Cleaned Token Count:',len(clean_tokens))\n",
    "\n",
    "#Create NLTK Text so we can use NLTK methods on the text (verified token count remained the same)\n",
    "text = nltk.Text(clean_tokens)\n",
    "\n",
    "#Calculate Lexical Diversity\n",
    "print('Lexical Diversity:',lexical_diversity(text))\n",
    "\n",
    "#Calculate Vocabulary Size\n",
    "print('Vocabulary Size:',len(set(text)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "287a2bec",
   "metadata": {},
   "source": [
    "#### Findings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7ea3268",
   "metadata": {},
   "source": [
    "As the reading level increased, the lexical diversity decreased. The lexical diversity of the 10-12th grade level book was 59.9% less than a 5th-6th grade level book in my selected examples.  I was quite surprised about this finding as you would intuitively think that higher grade level books would use a higher rate of unique words due to an expanded vocabulary.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee599770",
   "metadata": {},
   "source": [
    "### 4.\tAlso compare the vocabulary size of the same three texts. Explain whether the result was surprising.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6af31b50",
   "metadata": {},
   "source": [
    "(Vocabulary size is shown in the outputs above) The vocabulary size did increase across the three reading levels of the selected books. This was not surprising as you would expect higher grade level books to have an expanded vocabulary to match the increased vocabulary of the students."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a9e47dc",
   "metadata": {},
   "source": [
    "### 5.\tWrite a paragraph arguing whether vocabulary size and lexical diversity in combination could be a better measure of text difficulty (or reading level) than either measure is by itself."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac156a14",
   "metadata": {},
   "source": [
    "Lexical Diversity and Vocabulary Size have pitfalls when used individually and describe the bigger picture when used together. First, we will look at using only Vocabulary Size. Imagine an author wrote a short story for adults and a full length novel for kids. The short story is likely to have a much smaller Vocabulary Size due the length of the text even though it is meant for a higher reading level. Lexical Diversity would have helped us understand the complexity of the short story. Next, we will look at using only Lexical Diversity. Traditionally, higher level reading involves longer text. As seen in the three examples provided in the assignment, the length of text increased with the reading level at a higher rate than the Vocabulary Size. This left us with the unintuitive result of a lower Lexical Diversity for higher levels of reading. Having both Lexical Diversity and Vocabulary Size, we can more fully understand the complexity of the text."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ds_p37_r35]",
   "language": "python",
   "name": "conda-env-ds_p37_r35-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
